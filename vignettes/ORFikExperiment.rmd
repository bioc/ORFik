---
title: "Data management"
author: "Haakon Tjeldnes & Kornel Labun"
date: "`r BiocStyle::doc_date()`"
package: "`r pkg_ver('ORFik')`"
output: BiocStyle::html_document
vignette: >
  %\VignetteIndexEntry{ORFik Experiment}
  %\VignetteEngine{knitr::rmarkdown}
  %\VignetteEncoding{UTF-8}
---

# Introduction

Welcome to the introduction of data management with ORFik experiment. This vignette will walk you through how to work with large amounts of sequencing data effectively in ORFik.
`ORFik` is an R package containing various functions for analysis of RiboSeq, RNASeq and CageSeq data, we advice you to read ORFikOverview vignette, before starting this one.

## Motivation
NGS libraries are becoming more and more numerous. As a bioinformatician you often need to use many data-sets, like RNA-seq or ribo-seq together, to make some plots or statistics. A lot of things can go wrong when you scale up from just 1 data-set to many.

Another problem is also that annotations like gff and fasta files combined with the NGS data, must be separately loaded. Making it possible to use wrong annotation for the NGS data, or wrong chromosome naming as chr1 vs 1 etc.

## What is an ORFik experiment?
It is an object to massively simplify / error correcting your code, by having a table of all libraries and annotation of an experiment. That contains filepaths and info for each library / annotation files in the experiment. It also tries to guess grouping / types / pairs (paired end bam files etc.) by the file names. It is also a safety in that it verifies your experiments contain no duplicate files, or empty or non-accessible files.  Making it almost impossible to load the wrong data. In addition it checks chromosome naming of libraries and annotation, making sure you are not mixing chr1 vs 1 as name for chromosome 1 etc.

The main reason to represent your NGS data as an ORFik experiment will now be shown.

## Example of creating an ORFik experiment
First load ORFik
```{r eval = TRUE, echo = TRUE, message = FALSE}
library(ORFik)
```

Let's say we have a human experiment, containing annotation files (.gtf and .fasta genome) + Next generation sequencing libraries (NGS-data); RNA-seq, ribo-seq and CAGE.
```{r eval = TRUE, echo = TRUE}
# Read from (create.experiment() template)
# 1. Pick directory (normally a folder with bam / bed / wig files)
dir <- system.file("extdata", "", package = "ORFik")
list.files(dir)
```

```{r eval = TRUE, echo = TRUE}
# 2. Pick an experiment name
exper <- "ORFik"
# 3. Pick .gff/.gtf and fasta location
txdb <- system.file("extdata", "annotations.gtf", package = "ORFik")
fasta <- system.file("extdata", "genome.fasta", package = "ORFik")
template <- create.experiment(dir = dir,   # dir is the NGS files
                              exper,       # Experiment name
                              txdb = txdb, # gtf / gff / gff.db annotation
                              fa = fasta,  # Fasta genome
                              viewTemplate = FALSE)
data.frame(template)
```
You see from the template, it excludes files with .bai or .fai, .rdata etc, and only using data of NGS libraries, defined by argument (type).

You can also see it tries to guess library types, stages, replicates, condition etc. It will also try to auto-detect paired end bam files.
To fix the things it did not find, you either save the file and modify in Excel / Libre office, or do it directly in R.

Let's update the template to have correct tissue-fraction in one of the samples.
```{r eval = TRUE, echo = TRUE}
template$X5[6] <- "heart_valve" # <- fix non unique row (tissue fraction is heart valve)
# read experiment from template
df <- read.experiment(template)
```

To save it, do:
```{r eval = FALSE, echo = TRUE}
save.experiment(df, file = "path/to/save/experiment.csv")
```
You can then load the experiment whenever you need it.

# The experiment object
To see the object, just show it like this:
```{r eval = TRUE, echo = TRUE}
df
```
You see here that file paths are hidden, you can acces them like this:

If you have varying version of libraries, like p-shifted, bam, simplified wig files, you can get
filepaths to different version with this function.

```{r eval = TRUE, echo = TRUE}
filepath(df, type = "default")
```
## Loading all data in experiment
```{r eval = TRUE, echo = TRUE, warning = FALSE}
# First load experiment if not present
# We use our already loaded experiment: (df) here

# Load transcript annotation
txdb <- loadTxdb(df) # transcript annotation
# And now NGS data
outputLibs(df, chrStyle = seqlevelsStyle(txdb)) # Use txdb as seqlevelsStyle reference
```
By default all libraries are loaded into .GlobalEnv (global environment) with names decided by
columns in experiment, to see what the names will be, do:

```{r eval = TRUE, echo = TRUE}
bamVarName(df) #This will be the names:
```

You see here that the experiment name, "ORFik" is in the variable name
If you are only working on one experiment, you do not need to include the name, since
there is no possibility of duplicate naming (the experiment class validates all names are unique). 

To remove experiment name, do this:
```{r eval = TRUE, echo = TRUE}
df@expInVarName <- FALSE
bamVarName(df) #This will be the names:
```
Since we want NGS data names without "ORFik", let's remove them and load again.

```{r eval = TRUE, echo = TRUE}
df@expInVarName <- TRUE
remove.experiments(df)
df@expInVarName <- FALSE
outputLibs(df, chrStyle = seqlevelsStyle(txdb)) 
```

## Loading transcript regions
Let's say we want to load all leaders, cds and 3' UTRs that are longer than 30.
With ORFik experiment this is easy:
```{r eval = TRUE, echo = TRUE}
txNames <- filterTranscripts(txdb, minFiveUTR = 30,minCDS = 30, minThreeUTR = 30)
loadRegions(txdb, parts = c("leaders", "cds", "trailers"), names.keep = txNames)
```
The regions are now loaded into .GlobalEnv, only keeping transcripts from txNames.

## Plotting with ORFik experiments
Lets make a plot with coverage over mrna in just ribo-seq
```{r eval = TRUE, echo = TRUE, warning=FALSE}
transcriptWindow(leaders, cds, trailers, df[3,])
```

# P-site shifting experiment
If your experiment consists of Ribo-seq, you want to do p-site shifting.
```{r eval = FALSE, echo = TRUE, warning=FALSE}
shiftFootprintsByExperiment(df[df$libtype == "RFP",])
```
P-shifted ribo-seq will automaticly be stored as .wig (wiggle files for IGV and other genome browsers) and .ofst (ORFik serialized for R) files in a ./pshifted folder, relative to original libraries. 

To validate p-shifting, use shiftPlots. Here is an example from Bazzini et al. 2014 I made.
```{r eval = FALSE, echo = TRUE, warning=FALSE}
df.baz <- read.experiment("zf_bazzini14_RFP")
shiftPlots(df.baz, title = "Ribo-seq, zebrafish, Bazzini et al. 2014")
```
![p-site analysis](../inst/images/pshift_bazzini.png)

To see the shifts per library do:
```{r eval = FALSE, echo = TRUE, warning=FALSE}
shifts.load(df)
```

To see the location of pshifted files:
```{r eval = FALSE, echo = TRUE, warning=FALSE}
filepath(df[df$libtype == "RFP",], type = "pshifted")
```

To load p-shifted libraries, you can do:
```{r eval = FALSE, echo = TRUE, warning=FALSE}
outputLibs(df[df$libtype == "RFP",], type = "pshifted")
```

## Converting bam files to faster formats

Bam files are slow to load, and usually you don't need all the information
contained in a bam file.

Usually you convert to bed or wig files, but ORFik also support 2 formats
for much faster loading and use of data.

### ofst: ORFik serialized format
From the bam file store these columns as a serialized file:
seqname, start, cigar, strand, score (number of identical replicates for that read). 

This is the fastest format to use, loading time of 10GB Ribo-seq bam file reduced from minutes to ~ 1 second and ~ 20MB size.


### bedo: bed ORFik file
From the bam file store these columns as text file:
seqname, start, end (if not all widths are 1), strand, score (number of identical replicates for that read), size (size of cigar Ms according to reference)

The R object loaded from these files are GRanges, since cigar is not needed.

Loading time of 10GB Ribo-seq bam file reduced to ~ 10 seconds and ~ 100MB size.

### bedoc: bed ORFik file with cigar
From the bam file store these columns as text file:
seqname, cigar, start, strand, score (number of identical replicates for that read)

The R object loaded from these files are GAlignments or GAlignmentPairs, since cigar is needed.

Loading time of 10GB Ribo-seq bam file reduced to ~ 15 seconds and ~ 200MB size.

## ORFik QC report
ORFik also support a full QC report for post alignment statistics, correlation plots,
simplified libraries for plotting, meta coverage, ++.

```{r eval = FALSE, echo = TRUE, warning=FALSE}
QCreport(df)
```
The plots and statistics are saved to disc.
To see the statistics, you can do:
```{r eval = FALSE, echo = TRUE, warning=FALSE}
QCstats(df)
```
## Using the ORFik system in your script
Usually you want to do some operation on multiple data-sets. With ORFik experiment, 
this operation is simple.
There are 3 ways to run loops for the data:

1. if you know you have enough memory to load all data at once
```{r eval = FALSE, echo = TRUE, warning=FALSE}
outputLibs(df, type = "pshifted")
libs <- bamVarName(df) # <- here are names of the libs that were outputed
cds <- loadRegion(df, "cds")
# parallel loop
bplapply(libs, FUN = function(lib, cds) { 
    return(entropy(cds, get(lib)))
}, cds = cds)

```

2. Not loading data to global environment
```{r eval = FALSE, echo = TRUE, warning=FALSE}
files <- filepath(df, type = "pshifted")
cds <- loadRegion(df, "cds")
# parallel loop
res <- bplapply(files, FUN = function(file, cds) { 
    return(entropy(cds, fimport(file)))
}, cds = cds)

```

3. No parallel evaluation
```{r eval = FALSE, echo = TRUE, warning=FALSE}
files <- filepath(df, type = "pshifted")
cds <- loadRegion(df, "cds")
# Single thread loop
lapply(files, FUN = function(file, cds) { 
    return(entropy(cds, fimport(file)))
}, cds = cds)

```
